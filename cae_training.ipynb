{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from statistics import mean\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "from himodule.custom_classes import NasaDataset, LossAndMetric, WindowedLoader, CAE\n",
    "from himodule.ae_metrics import MAPE\n",
    "from himodule.normalisation import StandardScaler, MinMaxScaler\n",
    "from himodule.secondary_funcs import save_object, check_path, split_dataset, \\\n",
    "    seed_everything, split_anomaly_normal, split_anomaly_normal23\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "sns.set_theme(style='whitegrid', font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_window_size(window_size: int, kernel: int, stride: int):\n",
    "    return (window_size - kernel) // stride + 1\n",
    "\n",
    "def calculate_kernel_size(new_window_size: int, last_window_size: int, stride: int):\n",
    "    return last_window_size - (new_window_size - 1) * stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 37\n",
    "BATCH_SIZE = 64\n",
    "window_size = 30\n",
    "\n",
    "# Whole dataset loading\n",
    "train_dataset = NasaDataset('../datasets/clean_train_data.csv')\n",
    "# Separate normal and anomaly data\n",
    "train_dataset, anomaly_dataset = split_anomaly_normal23(train_dataset)\n",
    "seed_everything(SEED)\n",
    "# Get datasets for training and validation\n",
    "train_dataset, val_dataset = split_dataset(train_dataset, test_size=0.3)\n",
    "\n",
    "# Test dataset loading\n",
    "test_dataset = NasaDataset('../datasets/clean_test_data.csv')\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "try:\n",
    "    norm_name = repr(scaler).split(' ', maxsplit=2)[0].split('.')[-1]\n",
    "except IndexError:\n",
    "    norm_name = 'no_scaling'\n",
    "for idx, dtset in enumerate((train_dataset, val_dataset, test_dataset, anomaly_dataset)):\n",
    "    dtset.to(device)\n",
    "    if scaler:\n",
    "        if idx == 0:\n",
    "            scaler.fit(dtset.dataset)\n",
    "        dtset.dataset = scaler.transform(dtset.dataset)\n",
    "\n",
    "# Save trained scaler to use it in another files\n",
    "scaler_path = os.path.join('../scalers/', f'{norm_name}.pkl')\n",
    "if scaler and not os.path.exists(scaler_path):\n",
    "    save_object(scaler, scaler_path)\n",
    "\n",
    "seed_everything(SEED)\n",
    "train_loader = WindowedLoader(train_dataset, batch_size=BATCH_SIZE, window_size=window_size, for_conv=True)\n",
    "seed_everything(SEED)\n",
    "val_loader = WindowedLoader(train_dataset, batch_size=BATCH_SIZE, window_size=window_size, for_conv=True)\n",
    "\n",
    "seed_everything(SEED)\n",
    "test_loader = WindowedLoader(val_dataset, batch_size=BATCH_SIZE, window_size=window_size, for_conv=True)\n",
    "\n",
    "seed_everything(SEED)\n",
    "anomaly_loader = WindowedLoader(anomaly_dataset, batch_size=BATCH_SIZE, window_size=window_size, for_conv=True)\n",
    "\n",
    "print(f'Train: {len(train_dataset)}\\nValidation: {len(val_dataset)}\\nTest: {len(test_dataset)}')\n",
    "print(f'Anomaly: {len(anomaly_dataset)}')\n",
    "\n",
    "# Model creating\n",
    "params_dct = {\n",
    "    'conv_kernel': 3,\n",
    "    'conv_stride': 1,\n",
    "    'pool_kernel': 2,\n",
    "    'pool_stride': 1,\n",
    "    'unconv_stride': 1\n",
    "}\n",
    "\n",
    "conv_dct = {'kernel': params_dct['conv_kernel'], 'stride': params_dct['conv_stride']}\n",
    "pool_dct = {'kernel': params_dct['pool_kernel'], 'stride': params_dct['pool_stride']}\n",
    "\n",
    "window_sizes = [window_size]\n",
    "for dct in (conv_dct, pool_dct, conv_dct, pool_dct):\n",
    "    window_sizes.append(calculate_window_size(window_sizes[-1], **dct))\n",
    "\n",
    "input_shape = train_dataset.get_input_shape()\n",
    "layers_sizes = (input_shape*2,\n",
    "                input_shape*4,\n",
    "                window_sizes[-1]*input_shape*4,\n",
    "                window_sizes[-1]*input_shape*4 // 8,\n",
    "                window_sizes[-1]*input_shape*4 // 16)\n",
    "\n",
    "kernels = list()\n",
    "new_window_size = window_sizes[-1]\n",
    "for last_window_size in window_sizes[-3::-2]:\n",
    "    kernels.append(calculate_kernel_size(new_window_size, last_window_size, params_dct['unconv_stride']))\n",
    "    new_window_size = last_window_size\n",
    "params_dct.update({'unconv_kernels': kernels})\n",
    "\n",
    "seed_everything(SEED)\n",
    "model_cae = CAE(input_channels=input_shape, layers=layers_sizes, **params_dct).to(device)\n",
    "loss_func = nn.MSELoss()\n",
    "metric_func = MAPE()\n",
    "\n",
    "transform = torch.transpose\n",
    "transform_dct = {'dim0': 1, 'dim1': 2}\n",
    "get_loss_and_metric = LossAndMetric(loss_func, metric_func, scaler,\n",
    "                                    transform=transform, transform_dct=transform_dct)\n",
    "get_loss_and_metric_anomaly = LossAndMetric(nn.MSELoss(reduction='none'), metric_func, scaler,\n",
    "                                            transform=transform, transform_dct=transform_dct)\n",
    "\n",
    "optimiser = optim.AdamW(model_cae.parameters(),\n",
    "                       lr=1e-5)\n",
    "optimiser_name = repr(optimiser).split(' ', maxsplit=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: nn.Module, loader: DataLoader) -> Tuple[list, list]:\n",
    "\n",
    "    '''Returns losses and metrics of trained model.'''\n",
    "\n",
    "    with torch.no_grad():\n",
    "        losses = list()\n",
    "        metrics = list()\n",
    "        for dta in loader:\n",
    "            sample = dta['sensors']\n",
    "            sample = sample.to(device)\n",
    "            _, reconstruction = model(sample)\n",
    "\n",
    "            loss, metric = get_loss_and_metric(reconstruction, sample)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            metrics.append(metric.item())\n",
    "    return losses, metrics\n",
    "\n",
    "\n",
    "def plot_history(loss_history_df: pd.DataFrame, metric_history_df: pd.DataFrame,\n",
    "                 test_losses: float, test_metrics: float,\n",
    "                 ylabel_loss: str = None, ylabel_metric: str = None, save_path: str = None):\n",
    "    \n",
    "    '''Makes plot with 4 axes: training losses and metrics, test losses, validation losses and metrics, test metrics.'''\n",
    "\n",
    "    loss_history_df = loss_history_df.melt(ignore_index=False).iloc[1:]\n",
    "    metric_history_df = metric_history_df.melt(ignore_index=False).iloc[1:]\n",
    "    fig = plt.figure(layout=\"constrained\")\n",
    "    fig.set_size_inches(10, 10)\n",
    "\n",
    "    gs = GridSpec(2, 5, figure=fig)\n",
    "    ax1 = fig.add_subplot(gs[0, 0:4])\n",
    "    ax2 = fig.add_subplot(gs[0, 4])\n",
    "    \n",
    "    ax3 = fig.add_subplot(gs[1, 0:4])\n",
    "    ax4 = fig.add_subplot(gs[1, 4])\n",
    "\n",
    "    sns.lineplot(data=loss_history_df,\n",
    "                x=loss_history_df.index,\n",
    "                y='value',\n",
    "                hue='variable',\n",
    "                ax=ax1)\n",
    "    ax1.set_ylabel(ylabel_loss)\n",
    "    ax1.set_ylim(0, np.percentile(loss_history_df['value'].values, 99))\n",
    "\n",
    "    sns.boxplot(x=['test_loss']*len(test_losses),\n",
    "                y=test_losses,\n",
    "                color='g',\n",
    "                ax=ax2)\n",
    "    ax2.set_ylabel(None)\n",
    "\n",
    "    sns.lineplot(data=metric_history_df,\n",
    "                x=metric_history_df.index,\n",
    "                y='value',\n",
    "                hue='variable',\n",
    "                ax=ax3)\n",
    "    ax3.set_ylabel(ylabel_metric)\n",
    "\n",
    "    sns.boxplot(x=['test_metric']*len(test_metrics),\n",
    "                y=test_metrics,\n",
    "                color='g',\n",
    "                ax=ax4)\n",
    "    ax4.set_ylabel(None)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "history = list()\n",
    "\n",
    "# Model training on normal data only\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses = list()\n",
    "    train_metrics = list()\n",
    "    for dta in train_loader:\n",
    "        sample = dta['sensors']\n",
    "        sample = sample.to(device)\n",
    "        _, reconstruction = model_cae(sample)\n",
    "\n",
    "        loss, metric = get_loss_and_metric(reconstruction, sample)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        train_metrics.append(metric.item())\n",
    "    \n",
    "    val_losses, val_metrics = evaluate_model(model_cae, val_loader)\n",
    "    \n",
    "    train_loss, val_loss = mean(train_losses), mean(val_losses)\n",
    "    train_metrics, val_metrics = mean(train_metrics), mean(val_metrics)\n",
    "    history.append((epoch, train_loss, val_loss, train_metrics, val_metrics))\n",
    "    if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n",
    "        print(f'{epoch+1:>3}/{epochs:>3}: {train_loss=:.4f}, {val_loss=:.4f}, {train_metrics=:.4f}%, {val_metrics=:.4f}%')\n",
    "\n",
    "test_losses, test_metrics = evaluate_model(model_cae, test_loader)\n",
    "test_loss = mean(test_losses)\n",
    "test_metric = mean(test_metrics)\n",
    "print(f'\\n{test_loss=:.4f}, {test_metric=:.4f}%')\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------#\n",
    "# Model saving\n",
    "name = str(window_size)\n",
    "models_path = os.path.join('../Models/cae/', str(len(layers_sizes)+1))\n",
    "plot_path = os.path.join('../Plots/cae/', str(len(layers_sizes)+1))\n",
    "\n",
    "if True:\n",
    "    check_path(models_path)\n",
    "    torch.save(model_cae.state_dict(), os.path.join(models_path, f'{name}.pth'))\n",
    "\n",
    "\n",
    "if True:\n",
    "    # Make plots to evaluate model performance\n",
    "\n",
    "    columns = ('epoch', 'train_loss', 'val_loss', 'train_metric', 'val_metric')\n",
    "    total_history_df = pd.DataFrame(history, columns=columns).set_index('epoch')\n",
    "\n",
    "    check_path(plot_path)\n",
    "\n",
    "    loss_history_df = total_history_df.loc[:,('train_loss', 'val_loss')]\n",
    "    metric_history_df = total_history_df.loc[:,('train_metric', 'val_metric')]\n",
    "    plot_history(loss_history_df, metric_history_df, test_losses, test_metrics,\n",
    "                ylabel_loss='MSE', ylabel_metric='MAPE', save_path=os.path.join(plot_path, f'{name}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
