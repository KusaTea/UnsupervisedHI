{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from statistics import mean\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from himodule.custom_classes import NasaDataset, WindowedLoader, CAE\n",
    "from himodule.normalisation import StandardScaler, MinMaxScaler, ErrorScaler\n",
    "from himodule.secondary_funcs import save_object, load_object, check_path, split_dataset, \\\n",
    "    seed_everything, split_anomaly_normal, split_anomaly_normal23\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "\n",
    "sns.set_theme(style='whitegrid', font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_window_size(window_size: int, kernel: int, stride: int):\n",
    "    return (window_size - kernel) // stride + 1\n",
    "\n",
    "def calculate_kernel_size(new_window_size: int, last_window_size: int, stride: int):\n",
    "    return last_window_size - (new_window_size - 1) * stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 37\n",
    "batch_size = 20\n",
    "window_size = 20\n",
    "\n",
    "# Whole dataset loading\n",
    "train_dataset = NasaDataset('../datasets/clean_train_data.csv')\n",
    "\n",
    "test_dataset = NasaDataset('../datasets/clean_test_data.csv')\n",
    "\n",
    "scaler_path = '../scalers/MinMaxScaler.pkl'\n",
    "scaler = load_object('../scalers/MinMaxScaler.pkl')\n",
    "try:\n",
    "    norm_name = repr(scaler).split(' ', maxsplit=2)[0].split('.')[-1]\n",
    "except IndexError:\n",
    "    norm_name = 'no_scaling'\n",
    "\n",
    "train_dataset.to(device)\n",
    "train_dataset.dataset = scaler.transform(train_dataset.dataset)\n",
    "\n",
    "test_dataset.to(device)\n",
    "test_dataset.dataset = scaler.transform(test_dataset.dataset)\n",
    "\n",
    "seed_everything(seed)\n",
    "train_loader = WindowedLoader(train_dataset, batch_size=batch_size, window_size=window_size, for_conv=True)\n",
    "\n",
    "seed_everything(seed)\n",
    "test_loader = WindowedLoader(test_dataset, batch_size=batch_size, window_size=window_size, for_conv=True)\n",
    "\n",
    "print(f'Train: {len(train_dataset)}\\nTest: {len(test_dataset)}')\n",
    "\n",
    "input_shape = train_dataset.get_input_shape()\n",
    "layers_sizes = (input_shape*window_size//2, input_shape*window_size//4, input_shape*window_size//8)\n",
    "\n",
    "# Model creating\n",
    "params_dct = {\n",
    "    'conv_kernel': 3,\n",
    "    'conv_stride': 1,\n",
    "    'pool_kernel': 2,\n",
    "    'pool_stride': 1,\n",
    "    'unconv_stride': 1\n",
    "}\n",
    "\n",
    "conv_dct = {'kernel': params_dct['conv_kernel'], 'stride': params_dct['conv_stride']}\n",
    "pool_dct = {'kernel': params_dct['pool_kernel'], 'stride': params_dct['pool_stride']}\n",
    "\n",
    "window_sizes = [window_size]\n",
    "for dct in (conv_dct, pool_dct, conv_dct, pool_dct):\n",
    "    window_sizes.append(calculate_window_size(window_sizes[-1], **dct))\n",
    "\n",
    "input_shape = train_dataset.get_input_shape()\n",
    "layers_sizes = (input_shape*2,\n",
    "                input_shape*4,\n",
    "                window_sizes[-1]*input_shape*4,\n",
    "                window_sizes[-1]*input_shape*4 // 8,\n",
    "                window_sizes[-1]*input_shape*4 // 16)\n",
    "\n",
    "models_path = f'../Models/cae/{len(layers_sizes)+1}'\n",
    "\n",
    "kernels = list()\n",
    "new_window_size = window_sizes[-1]\n",
    "for last_window_size in window_sizes[-3::-2]:\n",
    "    kernels.append(calculate_kernel_size(new_window_size, last_window_size, params_dct['unconv_stride']))\n",
    "    new_window_size = last_window_size\n",
    "params_dct.update({'unconv_kernels': kernels})\n",
    "\n",
    "seed_everything(seed)\n",
    "model_cae = CAE(input_channels=input_shape, layers=layers_sizes, **params_dct).to(device)\n",
    "loss_func = nn.MSELoss()\n",
    "model_cae.load_state_dict(torch.load(os.path.join(models_path, f'{window_size}.pth')))\n",
    "model_cae = model_cae.to(device)\n",
    "loss_func = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_losses(loader: WindowedLoader, model_cae: CAE, loss_func, window_size: int, input_shape: int) -> list:\n",
    "    losses_dct = defaultdict(list)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for dta in loader:\n",
    "            sample = dta['sensors']\n",
    "            indeces = dta['indeces'].flatten()\n",
    "            _, reconstruction = model_cae(sample)\n",
    "            loss = loss_func(nn.Flatten()(reconstruction), nn.Flatten()(sample))\n",
    "            loss = loss.view(-1, input_shape, window_size).mean(dim=1).flatten()\n",
    "\n",
    "            for idx, ls in zip(indeces, loss):\n",
    "                losses_dct[idx.item()].append(ls.item())\n",
    "    \n",
    "    for id, lst in losses_dct.items():\n",
    "        losses_dct[id] = mean(lst)\n",
    "    return losses_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = get_losses(train_loader, model_cae, loss_func,\n",
    "                          window_size=window_size, input_shape=input_shape)\n",
    "test_losses = get_losses(test_loader, model_cae, loss_func,\n",
    "                          window_size=window_size, input_shape=input_shape)\n",
    "\n",
    "\n",
    "train_losses = torch.FloatTensor(tuple(train_losses.values()))\n",
    "test_losses = torch.FloatTensor(tuple(test_losses.values()))\n",
    "\n",
    "losses = {'train': train_losses, 'test': test_losses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays_path = '../HIs/cae/'\n",
    "for key in losses.keys():\n",
    "    check_path(os.path.join(arrays_path, key))\n",
    "\n",
    "error_scaler = ErrorScaler()\n",
    "\n",
    "for (sample_type, loss), dataset in zip(losses.items(), (train_dataset, test_dataset)):\n",
    "    for machine_id in dataset.machine_ids.unique():\n",
    "        indeces = dataset.get_indeces(machine_id)\n",
    "\n",
    "        arr = error_scaler.fit_transform(loss[indeces.cpu()]).numpy()\n",
    "        arr.tofile(os.path.join(arrays_path, sample_type, f'{int(machine_id)}.dat'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
