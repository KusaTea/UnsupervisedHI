{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from statistics import mean\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from himodule.custom_classes import NasaDataset, SimpleAE, AnomalyLoader, LossAndMetric\n",
    "from himodule.ae_metrics import MAPE\n",
    "from himodule.normalisation import StandardScaler, MinMaxScaler\n",
    "from himodule.secondary_funcs import save_object, load_object, check_path, split_dataset, seed_everything, split_anomaly_normal\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "sns.set_theme(style='whitegrid', font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 37\n",
    "\n",
    "# Whole dataset loading\n",
    "train_dataset = NasaDataset('../datasets/clean_train_data.csv')\n",
    "# Separate normal and anomaly data\n",
    "train_dataset, anomaly_dataset = split_anomaly_normal(train_dataset)\n",
    "seed_everything(seed)\n",
    "# Get datasets for training and validation\n",
    "train_dataset, val_dataset = split_dataset(train_dataset, test_size=0.3)\n",
    "\n",
    "# Test dataset loading\n",
    "test_dataset = NasaDataset('../datasets/clean_test_data.csv')\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "try:\n",
    "    norm_name = repr(scaler).split(' ', maxsplit=2)[0].split('.')[-1]\n",
    "except IndexError:\n",
    "    norm_name = 'no_scaling'\n",
    "for idx, dtset in enumerate((train_dataset, val_dataset, test_dataset, anomaly_dataset)):\n",
    "    dtset.to(device)\n",
    "    if scaler:\n",
    "        if idx == 0:\n",
    "            scaler.fit(dtset.dataset)\n",
    "        dtset.dataset = scaler.transform(dtset.dataset)\n",
    "\n",
    "# Save trained scaler to use it in another files\n",
    "scaler_path = os.path.join('../scalers/', f'{norm_name}.pkl')\n",
    "if scaler and not os.path.exists(scaler_path):\n",
    "    save_object(scaler, scaler_path)\n",
    "\n",
    "seed_everything(seed)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True, worker_init_fn=seed, generator=g)\n",
    "seed_everything(seed)\n",
    "val_loader = DataLoader(train_dataset, batch_size=20, shuffle=True, worker_init_fn=seed, generator=g)\n",
    "\n",
    "seed_everything(seed)\n",
    "test_loader = DataLoader(val_dataset, batch_size=20, shuffle=True, worker_init_fn=seed, generator=g)\n",
    "\n",
    "seed_everything(seed)\n",
    "anomaly_loader = AnomalyLoader(anomaly_dataset, batch_size=20)\n",
    "\n",
    "print(f'Train: {len(train_dataset)}\\nValidation: {len(val_dataset)}\\nTest: {len(test_dataset)}')\n",
    "print(f'Anomaly: {len(anomaly_dataset)}')\n",
    "\n",
    "input_shape = train_dataset.get_input_shape()\n",
    "layers_sizes = (8, 4, 2)\n",
    "\n",
    "# Model creating\n",
    "seed_everything(seed)\n",
    "model_ae = SimpleAE(input_shape, layers_sizes).to(device)\n",
    "loss_func = nn.MSELoss()\n",
    "metric_func = MAPE()\n",
    "get_loss_and_metric = LossAndMetric(loss_func, metric_func, scaler)\n",
    "get_loss_and_metric_anomaly = LossAndMetric(nn.MSELoss(reduction='none'), metric_func, scaler)\n",
    "optimiser = optim.AdamW(model_ae.parameters(),\n",
    "                       lr=1e-3)\n",
    "optimiser_name = repr(optimiser).split(' ', maxsplit=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: nn.Module, loader: DataLoader) -> Tuple[list, list]:\n",
    "\n",
    "    '''Returns losses and metrics of trained model.'''\n",
    "\n",
    "    with torch.no_grad():\n",
    "        losses = list()\n",
    "        metrics = list()\n",
    "        for dta in loader:\n",
    "            sample = dta['sensors']\n",
    "            sample = sample.to(device)\n",
    "            _, reconstruction = model(sample)\n",
    "\n",
    "            loss, metric = get_loss_and_metric(reconstruction, sample)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            metrics.append(metric.item())\n",
    "    return losses, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = '../Models/norm-anom/'\n",
    "\n",
    "epochs = 100\n",
    "history = list()\n",
    "\n",
    "# Model training on normal data only\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses = list()\n",
    "    train_metrics = list()\n",
    "    for dta in train_loader:\n",
    "        sample = dta['sensors']\n",
    "        sample = sample.to(device)\n",
    "        _, reconstruction = model_ae(sample)\n",
    "\n",
    "        loss, metric = get_loss_and_metric(reconstruction, sample)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        train_metrics.append(metric.item())\n",
    "    \n",
    "    val_losses, val_metrics = evaluate_model(model_ae, val_loader)\n",
    "    \n",
    "    train_loss, val_loss = mean(train_losses), mean(val_losses)\n",
    "    train_metrics, val_metrics = mean(train_metrics), mean(val_metrics)\n",
    "    history.append((epoch, train_loss, val_loss, train_metrics, val_metrics))\n",
    "    if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n",
    "        print(f'{epoch+1:>3}/{epochs:>3}: {train_loss=:.4f}, {val_loss=:.4f}, {train_metrics=:.4f}%, {val_metrics=:.4f}%')\n",
    "\n",
    "test_losses, test_metrics = evaluate_model(model_ae, test_loader)\n",
    "test_loss = mean(test_losses)\n",
    "test_metric = mean(test_metrics)\n",
    "print(f'\\n{test_loss=:.4f}, {test_metric=:.4f}%')\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    anomaly_losses = defaultdict(list)\n",
    "    anomaly_metrics = list()\n",
    "    for dta in anomaly_loader:\n",
    "        sample = dta['sensors']\n",
    "        machine_id = dta['machine_id'][0].item()\n",
    "        sample = sample.to(device)\n",
    "        _, reconstruction = model_ae(sample)\n",
    "        \n",
    "        loss, metric = get_loss_and_metric_anomaly(reconstruction, sample)\n",
    "        anomaly_losses[int(machine_id)].append(loss.mean(dim=1))\n",
    "        anomaly_metrics.append(metric)\n",
    "    \n",
    "    for key, values in anomaly_losses.items():\n",
    "        anomaly_losses[key] = torch.hstack(values)\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------#\n",
    "# Model saving\n",
    "check_path(models_path)\n",
    "torch.save(model_ae.state_dict(), os.path.join(models_path, f'{layers_sizes}.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_train_history(history: list) -> pd.DataFrame:\n",
    "\n",
    "    '''Transfrorms training history to format for plot.'''\n",
    "\n",
    "    columns = ('epoch', 'train_loss', 'val_loss', 'train_metric', 'val_metric')\n",
    "    total_history_df = pd.DataFrame(history, columns=columns).set_index('epoch')\n",
    "\n",
    "    history_df = total_history_df.loc[:,('train_loss', 'val_loss')].iloc[2:].melt(ignore_index=False)\n",
    "    return history_df\n",
    "\n",
    "\n",
    "def transform_anomaly_history(history: dict):\n",
    "\n",
    "    '''Transfrorms history of anomalies reconstructions to format for plot.'''\n",
    "\n",
    "    counter = 0\n",
    "    dfs = list()\n",
    "    for machine_id, losses in history.items():\n",
    "        if counter == 3:\n",
    "            yield pd.concat(dfs, axis=1, ignore_index=False).melt(ignore_index=False)\n",
    "            counter = 0\n",
    "            dfs = list()\n",
    "        \n",
    "        dfs.append(pd.Series(losses.cpu(), name=machine_id))\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "def plot_lines(ax: plt.Axes, history_df: pd.DataFrame, anomaly_df: dict, title: str = None):\n",
    "    sns.lineplot(data=history_df,\n",
    "                x=history_df.index,\n",
    "                y='value',\n",
    "                hue='variable',\n",
    "                ax=ax)\n",
    "\n",
    "    sns.lineplot(data=anomaly_df,\n",
    "                x=anomaly_df.index,\n",
    "                y='value',\n",
    "                hue='variable',\n",
    "                ax=ax)\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.set_title(title)\n",
    "\n",
    "    ax.set_ylabel('MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot lines of anomaly reconstruction MSE (for each machine individual line) and training history lines\n",
    "\n",
    "history_df = transform_train_history(history)\n",
    "\n",
    "for anomaly_df in transform_anomaly_history(anomaly_losses):\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(8, 4)\n",
    "    title = tuple(anomaly_df['variable'].unique())\n",
    "    plot_lines(ax, history_df, anomaly_df, title=title)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../Plots/history/norm-anom/{title}.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
