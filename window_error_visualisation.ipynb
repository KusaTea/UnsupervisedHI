{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from statistics import mean\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from himodule.custom_classes import NasaDataset, WindowedLoader, AEConstructor\n",
    "from himodule.normalisation import StandardScaler, MinMaxScaler, ErrorScaler\n",
    "from himodule.secondary_funcs import save_object, load_object, check_path, split_dataset, \\\n",
    "    seed_everything, split_anomaly_normal, split_anomaly_normal23\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "\n",
    "sns.set_theme(style='whitegrid', font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 37\n",
    "batch_size = 20\n",
    "window_size = 5\n",
    "\n",
    "# Whole dataset loading\n",
    "train_dataset = NasaDataset('../datasets/clean_train_data.csv')\n",
    "\n",
    "normal_dataset, anomaly_dataset = split_anomaly_normal23(train_dataset)\n",
    "\n",
    "scaler_path = '../scalers/MinMaxScaler.pkl'\n",
    "scaler = load_object('../scalers/MinMaxScaler.pkl')\n",
    "try:\n",
    "    norm_name = repr(scaler).split(' ', maxsplit=2)[0].split('.')[-1]\n",
    "except IndexError:\n",
    "    norm_name = 'no_scaling'\n",
    "\n",
    "normal_dataset.to(device)\n",
    "normal_dataset.dataset = scaler.transform(normal_dataset.dataset)\n",
    "\n",
    "anomaly_dataset.to(device)\n",
    "anomaly_dataset.dataset = scaler.transform(anomaly_dataset.dataset)\n",
    "\n",
    "seed_everything(seed)\n",
    "normal_loader = WindowedLoader(normal_dataset, batch_size=batch_size, window_size=window_size)\n",
    "\n",
    "seed_everything(seed)\n",
    "anomaly_loader = WindowedLoader(anomaly_dataset, batch_size=batch_size, window_size=window_size)\n",
    "\n",
    "print(f'Train: {len(normal_dataset)}')\n",
    "print(f'Anomaly: {len(anomaly_dataset)}')\n",
    "\n",
    "input_shape = normal_dataset.get_input_shape()\n",
    "layers_sizes = (input_shape*window_size//2, input_shape*window_size//4, input_shape*window_size//8)\n",
    "\n",
    "models_path = f'../Models/window/{len(layers_sizes)+1}'\n",
    "\n",
    "window_model = AEConstructor(input_shape, window_size=window_size, layers_sizes=layers_sizes)\n",
    "window_model.load_state_dict(torch.load(os.path.join(models_path, f'{layers_sizes}.pth')))\n",
    "window_model = window_model.to(device)\n",
    "loss_func = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_losses(loader: WindowedLoader, window_model: AEConstructor, loss_func, window_size: int, input_shape: int) -> list:\n",
    "    losses_dct = defaultdict(list)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for dta in loader:\n",
    "            sample = dta['sensors']\n",
    "            indeces = dta['indeces'].flatten()\n",
    "            _, reconstruction = window_model(sample)\n",
    "            loss = loss_func(nn.Flatten()(reconstruction), nn.Flatten()(sample))\n",
    "            loss = loss.view(-1, window_size, input_shape).mean(dim=2).flatten()\n",
    "\n",
    "            for idx, ls in zip(indeces, loss):\n",
    "                losses_dct[idx.item()].append(ls.item())\n",
    "    \n",
    "    for id, lst in losses_dct.items():\n",
    "        losses_dct[id] = mean(lst)\n",
    "    return losses_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_losses = get_losses(normal_loader, window_model, loss_func, window_size, input_shape)\n",
    "anomaly_losses = get_losses(anomaly_loader, window_model, loss_func, window_size, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_losses_df = pd.DataFrame((normal_dataset.machine_ids.cpu().numpy()[np.array(tuple(normal_losses.keys()))],\n",
    "                                 normal_losses.values(),\n",
    "                                 ['normal']*len(normal_losses)),\n",
    "                                 index=('machine_id', 'MSE', 'type')).T\n",
    "\n",
    "anomaly_losses_df = pd.DataFrame((anomaly_dataset.machine_ids.cpu().numpy()[np.array(tuple(anomaly_losses.keys()))],\n",
    "                                 anomaly_losses.values(),\n",
    "                                 ['anomaly']*len(anomaly_losses)),\n",
    "                                 index=('machine_id', 'MSE', 'type')).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot_df(normal_df: pd.DataFrame, anomaly_df: pd.DataFrame, machine_id: int):\n",
    "    n_df = normal_df[normal_df['machine_id'] == machine_id].loc[:,'MSE':'type']\n",
    "    a_df = anomaly_df[anomaly_df['machine_id'] == machine_id].loc[:,'MSE':'type']\n",
    "    \n",
    "    df = pd.concat((n_df, a_df), axis=0, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_path = '../Plots/window/norm-anom/'\n",
    "check_path(plots_path)\n",
    "\n",
    "for machine_id in normal_dataset.machine_ids.unique():\n",
    "    plot_df = get_plot_df(normal_losses_df, anomaly_losses_df, machine_id)\n",
    "\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(10, 5)\n",
    "\n",
    "    sns.lineplot(data=plot_df,\n",
    "                 x=plot_df.index,\n",
    "                 y='MSE',\n",
    "                 hue='type',\n",
    "                 palette=('g', 'r'))\n",
    "    \n",
    "    ax.legend(title=None)\n",
    "    ax.set_title(f'Machine id: {machine_id}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_path, f'{machine_id}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_path = '../Plots/window/norm-anom (scaled)/'\n",
    "check_path(plots_path)\n",
    "\n",
    "error_scaler = ErrorScaler()\n",
    "\n",
    "for machine_id in normal_dataset.machine_ids.unique():\n",
    "    plot_df = get_plot_df(normal_losses_df, anomaly_losses_df, machine_id)\n",
    "\n",
    "    plot_df['MSE'] = error_scaler.fit_transform(torch.from_numpy(plot_df['MSE'].values.astype(float))).cpu().numpy()\n",
    "\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(10, 5)\n",
    "\n",
    "    sns.lineplot(data=plot_df,\n",
    "                 x=plot_df.index,\n",
    "                 y='MSE',\n",
    "                 hue='type',\n",
    "                 palette=('g', 'r'))\n",
    "    \n",
    "    ax.legend(title=None)\n",
    "    ax.set_title(f'Machine id: {machine_id}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_path, f'{machine_id}.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HI Curves Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 37\n",
    "batch_size = 20\n",
    "window_size = 5\n",
    "\n",
    "# Whole dataset loading\n",
    "train_dataset = NasaDataset('../datasets/clean_train_data.csv')\n",
    "\n",
    "test_dataset = NasaDataset('../datasets/clean_test_data.csv')\n",
    "\n",
    "scaler_path = '../scalers/MinMaxScaler.pkl'\n",
    "scaler = load_object('../scalers/MinMaxScaler.pkl')\n",
    "try:\n",
    "    norm_name = repr(scaler).split(' ', maxsplit=2)[0].split('.')[-1]\n",
    "except IndexError:\n",
    "    norm_name = 'no_scaling'\n",
    "\n",
    "train_dataset.to(device)\n",
    "train_dataset.dataset = scaler.transform(train_dataset.dataset)\n",
    "\n",
    "test_dataset.to(device)\n",
    "test_dataset.dataset = scaler.transform(test_dataset.dataset)\n",
    "\n",
    "seed_everything(seed)\n",
    "train_loader = WindowedLoader(train_dataset, batch_size=batch_size, window_size=window_size)\n",
    "\n",
    "seed_everything(seed)\n",
    "test_loader = WindowedLoader(test_dataset, batch_size=batch_size, window_size=window_size)\n",
    "\n",
    "print(f'Train: {len(train_dataset)}\\nTest: {len(test_dataset)}')\n",
    "\n",
    "input_shape = train_dataset.get_input_shape()\n",
    "layers_sizes = (input_shape*window_size//2, input_shape*window_size//4, input_shape*window_size//8)\n",
    "\n",
    "models_path = f'../Models/window/{len(layers_sizes)+1}'\n",
    "\n",
    "window_model = AEConstructor(input_shape, window_size=window_size, layers_sizes=layers_sizes)\n",
    "window_model.load_state_dict(torch.load(os.path.join(models_path, f'{layers_sizes}.pth')))\n",
    "window_model = window_model.to(device)\n",
    "loss_func = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_losses(loader: WindowedLoader, window_model: AEConstructor, loss_func, window_size: int, input_shape: int) -> list:\n",
    "    losses_dct = defaultdict(list)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for dta in loader:\n",
    "            sample = dta['sensors']\n",
    "            indeces = dta['indeces'].flatten()\n",
    "            _, reconstruction = window_model(sample)\n",
    "            loss = loss_func(nn.Flatten()(reconstruction), nn.Flatten()(sample))\n",
    "            loss = loss.view(-1, window_size, input_shape).mean(dim=2).flatten()\n",
    "\n",
    "            for idx, ls in zip(indeces, loss):\n",
    "                losses_dct[idx.item()].append(ls.item())\n",
    "    \n",
    "    for id, lst in losses_dct.items():\n",
    "        losses_dct[id] = mean(lst)\n",
    "    return losses_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = get_losses(train_loader, window_model, loss_func,\n",
    "                          window_size=window_size, input_shape=input_shape)\n",
    "test_losses = get_losses(test_loader, window_model, loss_func,\n",
    "                          window_size=window_size, input_shape=input_shape)\n",
    "\n",
    "\n",
    "train_losses = torch.FloatTensor(tuple(train_losses.values()))\n",
    "test_losses = torch.FloatTensor(tuple(test_losses.values()))\n",
    "\n",
    "losses = {'train': train_losses, 'test': test_losses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays_path = '../HIs/'\n",
    "for key in losses.keys():\n",
    "    check_path(os.path.join(arrays_path, key))\n",
    "\n",
    "error_scaler = ErrorScaler()\n",
    "\n",
    "for (sample_type, loss), dataset in zip(losses.items(), (train_dataset, test_dataset)):\n",
    "    for machine_id in dataset.machine_ids.unique():\n",
    "        indeces = dataset.get_indeces(machine_id)\n",
    "\n",
    "        arr = error_scaler.fit_transform(loss[indeces.cpu()]).numpy()\n",
    "        arr.tofile(os.path.join(arrays_path, sample_type, f'{int(machine_id)}.dat'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
