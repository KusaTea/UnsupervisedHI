{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from statistics import mean\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "from himodule.custom_classes import NasaDataset, LossAndMetric\n",
    "from himodule.ae_metrics import MAPE\n",
    "from himodule.normalisation import StandardScaler, MinMaxScaler\n",
    "from himodule.secondary_funcs import save_object, load_object, check_path, split_dataset, \\\n",
    "    seed_everything, split_anomaly_normal, split_anomaly_normal23\n",
    "from himodule.linear_regression import LinearRegression\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "sns.set_theme(style='whitegrid', font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device='cuda'\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets(path: str):\n",
    "    arrays = dict()\n",
    "    for pth in glob.glob(os.path.join(path, '*.dat')):\n",
    "        arr = np.fromfile(pth)\n",
    "        arrays[int(pth.rsplit('\\\\', maxsplit=1)[-1][:-4])] = arr\n",
    "    \n",
    "    new_arrays = dict()\n",
    "    keys = sorted(list(arrays.keys()))\n",
    "    for key in keys:\n",
    "        new_arrays[key] = arrays[key]\n",
    "    return new_arrays\n",
    "\n",
    "def transform_targets(targets: dict):\n",
    "    targets = [np.array((targs, [machine_id]*len(targs))) for machine_id, targs in targets.items()]\n",
    "    targets = np.concatenate(targets, axis=1)[0]\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = transform_targets(get_targets('../Smoothed/train'))[:, None]\n",
    "test_targets = transform_targets(get_targets('../Smoothed/test'))[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 20631\n",
      "Test: 13096\n"
     ]
    }
   ],
   "source": [
    "seed = 37\n",
    "batch_size = 20\n",
    "window_size = 5\n",
    "\n",
    "# Whole dataset loading\n",
    "train_dataset = NasaDataset('../datasets/clean_train_data.csv', targets=train_targets)\n",
    "\n",
    "test_dataset = NasaDataset('../datasets/clean_test_data.csv', targets=test_targets)\n",
    "\n",
    "scaler_path = '../scalers/MinMaxScaler.pkl'\n",
    "scaler = load_object('../scalers/MinMaxScaler.pkl')\n",
    "try:\n",
    "    norm_name = repr(scaler).split(' ', maxsplit=2)[0].split('.')[-1]\n",
    "except IndexError:\n",
    "    norm_name = 'no_scaling'\n",
    "\n",
    "train_dataset.to(device)\n",
    "train_dataset.dataset = scaler.transform(train_dataset.dataset)\n",
    "\n",
    "test_dataset.to(device)\n",
    "test_dataset.dataset = scaler.transform(test_dataset.dataset)\n",
    "\n",
    "seed_everything(seed)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, generator=g)\n",
    "\n",
    "seed_everything(seed)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, generator=g)\n",
    "\n",
    "print(f'Train: {len(train_dataset)}\\nTest: {len(test_dataset)}')\n",
    "\n",
    "input_shape = train_dataset.get_input_shape()\n",
    "\n",
    "seed_everything(seed)\n",
    "linear_model = LinearRegression(input_shape).to(device)\n",
    "loss_func = nn.MSELoss()\n",
    "metric_func = MAPE()\n",
    "optimiser = optim.Adam(linear_model.parameters(),\n",
    "                       lr=1e-3)\n",
    "optimiser_name = repr(optimiser).split(' ', maxsplit=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10/100: train_loss=0.0140, train_metrics=12.2638%\n",
      " 20/100: train_loss=0.0128, train_metrics=11.7513%\n",
      " 30/100: train_loss=0.0122, train_metrics=11.5041%\n",
      " 40/100: train_loss=0.0119, train_metrics=11.3690%\n",
      " 50/100: train_loss=0.0118, train_metrics=11.2950%\n",
      " 60/100: train_loss=0.0117, train_metrics=11.2558%\n",
      " 70/100: train_loss=0.0116, train_metrics=11.2348%\n",
      " 80/100: train_loss=0.0116, train_metrics=11.2237%\n",
      " 90/100: train_loss=0.0116, train_metrics=11.2177%\n",
      "100/100: train_loss=0.0116, train_metrics=11.2147%\n",
      "\n",
      "Test: mean(test_losses)=0.0417, mean(test_metrics)=27.5697%\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "history = list()\n",
    "\n",
    "# Model training on normal data only\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses = list()\n",
    "    train_metrics = list()\n",
    "    for dta in train_loader:\n",
    "        sample = dta['sensors']\n",
    "        hi_target = dta['targets']\n",
    "        sample = sample.to(device)\n",
    "        hi = linear_model(sample)\n",
    "\n",
    "        loss = loss_func(hi, hi_target)\n",
    "        metric = metric_func(hi, hi_target)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        train_metrics.append(metric.item())\n",
    "    \n",
    "    train_loss = mean(train_losses)\n",
    "    train_metrics = mean(train_metrics)\n",
    "    history.append((epoch, train_loss, train_metrics))\n",
    "    if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n",
    "        print(f'{epoch+1:>3}/{epochs:>3}: {train_loss=:.4f}, {train_metrics=:.4f}%')\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_losses = list()\n",
    "    test_metrics = list()\n",
    "    for dta in test_loader:\n",
    "        sample = dta['sensors']\n",
    "        hi_target = dta['targets']\n",
    "        sample = sample.to(device)\n",
    "        hi = linear_model(sample)\n",
    "\n",
    "        loss = loss_func(hi, hi_target)\n",
    "        metric = metric_func(hi, hi_target)\n",
    "\n",
    "        test_losses.append(loss.item())\n",
    "        test_metrics.append(metric.item())\n",
    "    \n",
    "    print(f'\\nTest: {mean(test_losses)=:.4f}, {mean(test_metrics)=:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13096, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_targets[:, None].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
