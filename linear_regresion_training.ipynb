{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from statistics import mean\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "from himodule.custom_classes import NasaDataset, LossAndMetric\n",
    "from himodule.ae_metrics import MAPE\n",
    "from himodule.normalisation import StandardScaler, MinMaxScaler\n",
    "from himodule.secondary_funcs import save_object, load_object, check_path, split_dataset, \\\n",
    "    seed_everything, split_anomaly_normal, split_anomaly_normal23\n",
    "from himodule.linear_regression import LinearRegression\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "sns.set_theme(style='whitegrid', font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets(path: str):\n",
    "    arrays = dict()\n",
    "    for pth in glob.glob(os.path.join(path, '*.dat')):\n",
    "        arr = np.fromfile(pth)\n",
    "        arrays[int(pth.rsplit('\\\\', maxsplit=1)[-1][:-4])] = arr\n",
    "    \n",
    "    new_arrays = dict()\n",
    "    keys = sorted(list(arrays.keys()))\n",
    "    for key in keys:\n",
    "        new_arrays[key] = arrays[key]\n",
    "    return new_arrays\n",
    "\n",
    "def transform_targets(targets: dict):\n",
    "    targets = [np.array((targs, [machine_id]*len(targs))) for machine_id, targs in targets.items()]\n",
    "    targets = np.concatenate(targets, axis=1)[0]\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_train_dataset(dataset: NasaDataset, step: int = 10):\n",
    "    new_indeces = list()\n",
    "    for machine_id in dataset.machine_ids.unique():\n",
    "        machine_id = int(machine_id.item())\n",
    "        indeces = dataset.get_indeces(machine_id)\n",
    "        hi_less7, hi_greater7 = (indeces[(dataset.targets[indeces] <= 0.7).flatten()],\n",
    "                                 indeces[(dataset.targets[indeces] > 0.7).flatten()])\n",
    "        hi_greater7_mask = torch.BoolTensor([True]*len(hi_greater7))\n",
    "        hi_greater7_mask[::step] = False\n",
    "        indeces = torch.concat((hi_greater7[hi_greater7_mask], hi_less7))\n",
    "        new_indeces.append(indeces)\n",
    "\n",
    "    new_indeces = torch.concat(new_indeces)\n",
    "    \n",
    "    return NasaDataset(dataset_dict={\n",
    "        'sensors': dataset.dataset[new_indeces],\n",
    "        'rul': dataset.ruls[new_indeces],\n",
    "        'machine_id': dataset.machine_ids[new_indeces]\n",
    "    }, targets=dataset.targets[new_indeces])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = transform_targets(get_targets('../Smoothed/cae/train'))[:, None]\n",
    "test_targets = transform_targets(get_targets('../Smoothed/cae/test'))[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 37\n",
    "batch_size = 20\n",
    "window_size = 5\n",
    "\n",
    "# Whole dataset loading\n",
    "train_dataset = NasaDataset('../datasets/clean_train_data.csv', targets=train_targets)\n",
    "\n",
    "seed_everything(seed)\n",
    "train_dataset, val_dataset = split_dataset(train_dataset, test_size=.25)\n",
    "train_dataset = sparse_train_dataset(train_dataset, step=5)\n",
    "\n",
    "test_dataset = NasaDataset('../datasets/clean_test_data.csv', targets=test_targets)\n",
    "\n",
    "scaler_path = '../scalers/MinMaxScaler.pkl'\n",
    "scaler = load_object('../scalers/MinMaxScaler.pkl')\n",
    "try:\n",
    "    norm_name = repr(scaler).split(' ', maxsplit=2)[0].split('.')[-1]\n",
    "except IndexError:\n",
    "    norm_name = 'no_scaling'\n",
    "\n",
    "for dataset in (train_dataset, val_dataset, test_dataset):\n",
    "    dataset.to(device)\n",
    "    dataset.dataset = scaler.transform(dataset.dataset)\n",
    "\n",
    "seed_everything(seed)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, generator=g)\n",
    "\n",
    "seed_everything(seed)\n",
    "val_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, generator=g)\n",
    "\n",
    "seed_everything(seed)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, generator=g)\n",
    "\n",
    "print(f'Train: {len(train_dataset)}\\nValidation: {len(val_dataset)}\\nTest: {len(test_dataset)}')\n",
    "\n",
    "input_shape = train_dataset.get_input_shape()\n",
    "\n",
    "seed_everything(seed)\n",
    "linear_model = LinearRegression(input_shape).to(device)\n",
    "loss_func = nn.MSELoss()\n",
    "metric_func = MAPE()\n",
    "optimiser = optim.AdamW(linear_model.parameters(),\n",
    "                       lr=1e-3)\n",
    "optimiser_name = repr(optimiser).split(' ', maxsplit=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "history = list()\n",
    "\n",
    "# Model training on normal data only\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses = list()\n",
    "    train_metrics = list()\n",
    "    for dta in train_loader:\n",
    "        sample = dta['sensors']\n",
    "        hi_target = dta['targets']\n",
    "        sample = sample.to(device)\n",
    "        hi = linear_model(sample)\n",
    "\n",
    "        loss = loss_func(hi, hi_target)\n",
    "        metric = metric_func(hi, hi_target)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        train_metrics.append(metric.item())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_losses = list()\n",
    "        val_metrics = list()\n",
    "        for dta in test_loader:\n",
    "            sample = dta['sensors']\n",
    "            hi_target = dta['targets']\n",
    "            sample = sample.to(device)\n",
    "            hi = linear_model(sample)\n",
    "\n",
    "            loss = loss_func(hi, hi_target)\n",
    "            metric = metric_func(hi, hi_target)\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "            val_metrics.append(metric.item())\n",
    "    \n",
    "    train_loss, val_loss = mean(train_losses), mean(val_losses)\n",
    "    train_metrics, val_metrics = mean(train_metrics), mean(val_metrics)\n",
    "    history.append((epoch, train_loss, val_loss, train_metrics, val_metrics))\n",
    "    if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n",
    "        print(f'{epoch+1:>3}/{epochs:>3}: {train_loss=:.4f}, {val_loss=:.4f}, {train_metrics=:.4f}%, {val_metrics=:.4f}%')\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_losses = list()\n",
    "    test_metrics = list()\n",
    "    for dta in test_loader:\n",
    "        sample = dta['sensors']\n",
    "        hi_target = dta['targets']\n",
    "        sample = sample.to(device)\n",
    "        hi = linear_model(sample)\n",
    "\n",
    "        loss = loss_func(hi, hi_target)\n",
    "        metric = metric_func(hi, hi_target)\n",
    "\n",
    "        test_losses.append(loss.item())\n",
    "        test_metrics.append(metric.item())\n",
    "    \n",
    "    print(f'\\nTest: {mean(test_losses)=:.4f}, {mean(test_metrics)=:.4f}%')\n",
    "\n",
    "#--------------------------------#\n",
    "model_path = '../LinearRegression'\n",
    "\n",
    "if True:\n",
    "    check_path(model_path)\n",
    "    torch.save(linear_model.state_dict(), os.path.join(model_path, 'regression_cae.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 37\n",
    "batch_size = 20\n",
    "window_size = 5\n",
    "\n",
    "train_targets = transform_targets(get_targets('../Smoothed/cae/train'))[:, None]\n",
    "test_targets = transform_targets(get_targets('../Smoothed/cae/test'))[:, None]\n",
    "\n",
    "train_dataset = NasaDataset('../datasets/clean_train_data.csv', targets=train_targets)\n",
    "seed_everything(seed)\n",
    "_, val_dataset = split_dataset(train_dataset, .25)\n",
    "\n",
    "test_dataset = NasaDataset('../datasets/clean_test_data.csv', targets=test_targets)\n",
    "\n",
    "scaler_path = '../scalers/MinMaxScaler.pkl'\n",
    "scaler = load_object('../scalers/MinMaxScaler.pkl')\n",
    "try:\n",
    "    norm_name = repr(scaler).split(' ', maxsplit=2)[0].split('.')[-1]\n",
    "except IndexError:\n",
    "    norm_name = 'no_scaling'\n",
    "\n",
    "for dataset in (test_dataset, val_dataset):\n",
    "    dataset.to(device)\n",
    "    dataset.dataset = scaler.transform(dataset.dataset)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "seed_everything(seed)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, generator=g)\n",
    "\n",
    "seed_everything(seed)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, generator=g)\n",
    "\n",
    "print(f'Test: {len(test_dataset)}\\nValidation: {len(val_dataset)}')\n",
    "\n",
    "input_shape = test_dataset.get_input_shape()\n",
    "\n",
    "model_path = '../LinearRegression'\n",
    "seed_everything(seed)\n",
    "linear_model = LinearRegression(input_shape).to(device)\n",
    "linear_model.load_state_dict(torch.load(os.path.join(model_path, f'regression_cae.pth')))\n",
    "linear_model = linear_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(loader: DataLoader, dataset: NasaDataset, linear_model: LinearRegression):\n",
    "    predictions = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for dta in loader:\n",
    "            sample = dta['sensors']\n",
    "            sample = sample.to(device)\n",
    "            hi = linear_model(sample)\n",
    "\n",
    "            predictions.append(hi)\n",
    "\n",
    "    predictions = torch.vstack(predictions)\n",
    "    plot_arr = torch.concat((dataset.machine_ids[:, None], dataset.targets, predictions), dim=1)\n",
    "    \n",
    "    return plot_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot_df(plot_arr: torch.Tensor, machine_id: int) -> pd.DataFrame:\n",
    "    arr = plot_arr[plot_arr[:,0] == machine_id][:, 1:]\n",
    "    plot_df = pd.DataFrame(arr.cpu(), columns=('true', 'predicted')).melt(ignore_index=False)\n",
    "    return plot_df\n",
    "\n",
    "def make_plot(plot_df: pd.DataFrame, machine_id: int = None, save_path: str = None):\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(10, 5)\n",
    "\n",
    "    sns.lineplot(data=plot_df,\n",
    "                 x=plot_df.index,\n",
    "                 y='value',\n",
    "                 hue='variable',\n",
    "                 ax=ax)\n",
    "    \n",
    "    ax.set_ylabel('Health Index')\n",
    "    ax.set_xlabel('Cycle')\n",
    "    ax.set_title(f'Machine id: {machine_id}')\n",
    "    ax.legend(title=None)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arr = get_predictions(test_loader, test_dataset, linear_model)\n",
    "\n",
    "plots_path = '../Plots/HIs/cae/test'\n",
    "check_path(plots_path)\n",
    "\n",
    "for machine_id in test_dataset.machine_ids.unique():\n",
    "    machine_id = int(machine_id.item())\n",
    "    plot_df = get_plot_df(plot_arr, machine_id)\n",
    "    make_plot(plot_df, machine_id, save_path=os.path.join(plots_path, f'{machine_id}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arr = get_predictions(val_loader, val_dataset, linear_model)\n",
    "\n",
    "plots_path = '../Plots/HIs/cae/validation'\n",
    "check_path(plots_path)\n",
    "\n",
    "for machine_id in val_dataset.machine_ids.unique():\n",
    "    machine_id = int(machine_id.item())\n",
    "    plot_df = get_plot_df(plot_arr, machine_id)\n",
    "    make_plot(plot_df, machine_id, save_path=os.path.join(plots_path, f'{machine_id}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
