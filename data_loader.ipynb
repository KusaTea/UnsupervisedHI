{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data from txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataset(train_path: str, test_path: str, columns_names: str, to_path: str):\n",
    "    with h5py.File(to_path, 'w') as to_file:\n",
    "        to_file.attrs['columns'] = columns_names\n",
    "\n",
    "        for path in tqdm((train_path, test_path), file=sys.stdout):\n",
    "            group_name = path.rsplit('/', maxsplit=1)[-1].split('_', maxsplit=1)[0]\n",
    "            group = to_file.create_group(name=group_name)\n",
    "            \n",
    "            with open(path, 'r') as from_file:\n",
    "                data = list()\n",
    "                last_id = None\n",
    "                \n",
    "                for row in from_file:\n",
    "                    row = [float(num) if '.' in num else int(num) for num in row.strip().split(' ')]\n",
    "\n",
    "                    if last_id and last_id != row[0]:\n",
    "                        group.create_dataset(name=str(last_id), data=np.array(data), dtype=np.float32)\n",
    "\n",
    "                    data.append(row[2:])\n",
    "                    last_id = row[0]\n",
    "                \n",
    "                group.create_dataset(name=str(last_id), data=np.array(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = '../datasets/test_FD001.txt'\n",
    "train_path = '../datasets/train_FD001.txt'\n",
    "\n",
    "columns_names = ('1',\n",
    "                 '2',\n",
    "                 '3',\n",
    "                 'T2',\n",
    "                 'T24',\n",
    "                 'T30',\n",
    "                 'T50',\n",
    "                 'P2',\n",
    "                 'P15',\n",
    "                 'P30',\n",
    "                 'Nf',\n",
    "                 'Nc',\n",
    "                 'epr',\n",
    "                 'Ps30',\n",
    "                 'phi',\n",
    "                 'NRf',\n",
    "                 'NRc',\n",
    "                 'BPR',\n",
    "                 'farB',\n",
    "                 'htBleed',\n",
    "                 'Nf_dmd',\n",
    "                 'PCNfR_dmd',\n",
    "                 'W31',\n",
    "                 'W32')\n",
    "\n",
    "to_path = '../datasets/NASA.hdf'\n",
    "\n",
    "if not os.path.exists(to_path):\n",
    "    construct_dataset(train_path, test_path, columns_names, to_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NasaDatasset_train(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path: str, transform = None):\n",
    "        self.hdf_file = h5py.File(dataset_path, 'r')['train']\n",
    "        self.transfrom = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class NasaDataset_test(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path: str, transform = None):\n",
    "        self.hdf_file = h5py.File(dataset_path, 'r')['test']\n",
    "        self.transfrom = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.hdf_file)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        sensors_data = self.hdf_file[str(idx+1)][:]\n",
    "\n",
    "        if self.transfrom:\n",
    "            sensors_data = self.transfrom(sensors_data)\n",
    "\n",
    "        return sensors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NasaDataset_test('../datasets/NASA.hdf', 'train')\n",
    "loader = DataLoader(dataset, batch_size=5, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
